{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3162,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009487666034155597,
      "grad_norm": 16956.0234375,
      "learning_rate": 0.000299146110056926,
      "loss": 3.1786,
      "step": 10
    },
    {
      "epoch": 0.018975332068311195,
      "grad_norm": 17460.625,
      "learning_rate": 0.0002981973434535104,
      "loss": 3.3635,
      "step": 20
    },
    {
      "epoch": 0.028462998102466792,
      "grad_norm": 15505.400390625,
      "learning_rate": 0.00029724857685009483,
      "loss": 3.1043,
      "step": 30
    },
    {
      "epoch": 0.03795066413662239,
      "grad_norm": 13139.1611328125,
      "learning_rate": 0.0002962998102466793,
      "loss": 3.1726,
      "step": 40
    },
    {
      "epoch": 0.04743833017077799,
      "grad_norm": 19630.619140625,
      "learning_rate": 0.0002953510436432637,
      "loss": 3.0462,
      "step": 50
    },
    {
      "epoch": 0.056925996204933584,
      "grad_norm": 26639.7890625,
      "learning_rate": 0.0002944022770398482,
      "loss": 2.9535,
      "step": 60
    },
    {
      "epoch": 0.06641366223908918,
      "grad_norm": 22922.875,
      "learning_rate": 0.0002934535104364326,
      "loss": 3.4645,
      "step": 70
    },
    {
      "epoch": 0.07590132827324478,
      "grad_norm": 27970.537109375,
      "learning_rate": 0.00029250474383301706,
      "loss": 3.184,
      "step": 80
    },
    {
      "epoch": 0.08538899430740038,
      "grad_norm": 19267.205078125,
      "learning_rate": 0.0002915559772296015,
      "loss": 2.8727,
      "step": 90
    },
    {
      "epoch": 0.09487666034155598,
      "grad_norm": 23281.58984375,
      "learning_rate": 0.00029060721062618594,
      "loss": 3.0028,
      "step": 100
    },
    {
      "epoch": 0.10436432637571158,
      "grad_norm": 40149.27734375,
      "learning_rate": 0.0002896584440227704,
      "loss": 3.0443,
      "step": 110
    },
    {
      "epoch": 0.11385199240986717,
      "grad_norm": 16625.962890625,
      "learning_rate": 0.0002887096774193548,
      "loss": 2.9115,
      "step": 120
    },
    {
      "epoch": 0.12333965844402277,
      "grad_norm": 26792.09765625,
      "learning_rate": 0.00028776091081593924,
      "loss": 2.9058,
      "step": 130
    },
    {
      "epoch": 0.13282732447817835,
      "grad_norm": 25105.55078125,
      "learning_rate": 0.0002868121442125237,
      "loss": 2.8924,
      "step": 140
    },
    {
      "epoch": 0.14231499051233396,
      "grad_norm": 23383.95703125,
      "learning_rate": 0.0002858633776091081,
      "loss": 2.6898,
      "step": 150
    },
    {
      "epoch": 0.15180265654648956,
      "grad_norm": 33937.55078125,
      "learning_rate": 0.0002849146110056926,
      "loss": 2.9716,
      "step": 160
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 38101.0703125,
      "learning_rate": 0.000283965844402277,
      "loss": 2.9176,
      "step": 170
    },
    {
      "epoch": 0.17077798861480076,
      "grad_norm": 37649.0703125,
      "learning_rate": 0.0002830170777988615,
      "loss": 2.9507,
      "step": 180
    },
    {
      "epoch": 0.18026565464895636,
      "grad_norm": 42825.28125,
      "learning_rate": 0.0002820683111954459,
      "loss": 2.9079,
      "step": 190
    },
    {
      "epoch": 0.18975332068311196,
      "grad_norm": 29026.20703125,
      "learning_rate": 0.0002811195445920303,
      "loss": 3.3187,
      "step": 200
    },
    {
      "epoch": 0.19924098671726756,
      "grad_norm": 45726.84375,
      "learning_rate": 0.00028017077798861477,
      "loss": 2.7908,
      "step": 210
    },
    {
      "epoch": 0.20872865275142316,
      "grad_norm": 29462.669921875,
      "learning_rate": 0.00027922201138519924,
      "loss": 2.8254,
      "step": 220
    },
    {
      "epoch": 0.21821631878557876,
      "grad_norm": 31086.806640625,
      "learning_rate": 0.00027827324478178365,
      "loss": 2.9059,
      "step": 230
    },
    {
      "epoch": 0.22770398481973433,
      "grad_norm": 39775.58984375,
      "learning_rate": 0.0002773244781783681,
      "loss": 2.7156,
      "step": 240
    },
    {
      "epoch": 0.23719165085388993,
      "grad_norm": 42012.765625,
      "learning_rate": 0.00027637571157495253,
      "loss": 2.7397,
      "step": 250
    },
    {
      "epoch": 0.24667931688804554,
      "grad_norm": 24940.931640625,
      "learning_rate": 0.000275426944971537,
      "loss": 2.6114,
      "step": 260
    },
    {
      "epoch": 0.25616698292220114,
      "grad_norm": 35302.49609375,
      "learning_rate": 0.0002744781783681214,
      "loss": 2.9444,
      "step": 270
    },
    {
      "epoch": 0.2656546489563567,
      "grad_norm": 39281.87109375,
      "learning_rate": 0.00027352941176470583,
      "loss": 2.5803,
      "step": 280
    },
    {
      "epoch": 0.27514231499051234,
      "grad_norm": 38651.78125,
      "learning_rate": 0.0002725806451612903,
      "loss": 2.5897,
      "step": 290
    },
    {
      "epoch": 0.2846299810246679,
      "grad_norm": 38852.13671875,
      "learning_rate": 0.0002716318785578747,
      "loss": 2.893,
      "step": 300
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 39935.6875,
      "learning_rate": 0.0002706831119544592,
      "loss": 2.7431,
      "step": 310
    },
    {
      "epoch": 0.3036053130929791,
      "grad_norm": 31121.091796875,
      "learning_rate": 0.00026973434535104365,
      "loss": 2.7586,
      "step": 320
    },
    {
      "epoch": 0.31309297912713474,
      "grad_norm": 38535.2265625,
      "learning_rate": 0.00026878557874762806,
      "loss": 2.3313,
      "step": 330
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 50524.06640625,
      "learning_rate": 0.00026783681214421253,
      "loss": 2.7304,
      "step": 340
    },
    {
      "epoch": 0.33206831119544594,
      "grad_norm": 30517.947265625,
      "learning_rate": 0.00026688804554079695,
      "loss": 2.7191,
      "step": 350
    },
    {
      "epoch": 0.3415559772296015,
      "grad_norm": 37488.76171875,
      "learning_rate": 0.00026593927893738136,
      "loss": 2.7367,
      "step": 360
    },
    {
      "epoch": 0.3510436432637571,
      "grad_norm": 28555.7265625,
      "learning_rate": 0.00026499051233396583,
      "loss": 2.625,
      "step": 370
    },
    {
      "epoch": 0.3605313092979127,
      "grad_norm": 32605.716796875,
      "learning_rate": 0.00026404174573055024,
      "loss": 2.6168,
      "step": 380
    },
    {
      "epoch": 0.3700189753320683,
      "grad_norm": 48836.1328125,
      "learning_rate": 0.0002630929791271347,
      "loss": 2.654,
      "step": 390
    },
    {
      "epoch": 0.3795066413662239,
      "grad_norm": 46018.9296875,
      "learning_rate": 0.0002621442125237191,
      "loss": 2.6146,
      "step": 400
    },
    {
      "epoch": 0.3889943074003795,
      "grad_norm": 43298.85546875,
      "learning_rate": 0.0002611954459203036,
      "loss": 2.7042,
      "step": 410
    },
    {
      "epoch": 0.3984819734345351,
      "grad_norm": 56624.6484375,
      "learning_rate": 0.000260246679316888,
      "loss": 2.9666,
      "step": 420
    },
    {
      "epoch": 0.4079696394686907,
      "grad_norm": 48075.43359375,
      "learning_rate": 0.0002592979127134725,
      "loss": 2.4726,
      "step": 430
    },
    {
      "epoch": 0.4174573055028463,
      "grad_norm": 32080.43359375,
      "learning_rate": 0.0002583491461100569,
      "loss": 2.4701,
      "step": 440
    },
    {
      "epoch": 0.4269449715370019,
      "grad_norm": 48359.75390625,
      "learning_rate": 0.00025740037950664136,
      "loss": 2.6368,
      "step": 450
    },
    {
      "epoch": 0.4364326375711575,
      "grad_norm": 46632.26171875,
      "learning_rate": 0.00025645161290322577,
      "loss": 3.0762,
      "step": 460
    },
    {
      "epoch": 0.4459203036053131,
      "grad_norm": 32467.28515625,
      "learning_rate": 0.00025550284629981024,
      "loss": 2.6233,
      "step": 470
    },
    {
      "epoch": 0.45540796963946867,
      "grad_norm": 44543.6953125,
      "learning_rate": 0.00025455407969639465,
      "loss": 2.5112,
      "step": 480
    },
    {
      "epoch": 0.4648956356736243,
      "grad_norm": 34669.7421875,
      "learning_rate": 0.0002536053130929791,
      "loss": 2.6395,
      "step": 490
    },
    {
      "epoch": 0.47438330170777987,
      "grad_norm": 43604.9921875,
      "learning_rate": 0.00025265654648956354,
      "loss": 2.8407,
      "step": 500
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 37956.82421875,
      "learning_rate": 0.00025170777988614795,
      "loss": 2.369,
      "step": 510
    },
    {
      "epoch": 0.49335863377609107,
      "grad_norm": 43986.1328125,
      "learning_rate": 0.0002507590132827324,
      "loss": 2.7539,
      "step": 520
    },
    {
      "epoch": 0.5028462998102466,
      "grad_norm": 39571.7265625,
      "learning_rate": 0.00024981024667931683,
      "loss": 2.7079,
      "step": 530
    },
    {
      "epoch": 0.5123339658444023,
      "grad_norm": 25566.3125,
      "learning_rate": 0.0002488614800759013,
      "loss": 2.7699,
      "step": 540
    },
    {
      "epoch": 0.5218216318785579,
      "grad_norm": 35077.76171875,
      "learning_rate": 0.00024791271347248577,
      "loss": 2.6701,
      "step": 550
    },
    {
      "epoch": 0.5313092979127134,
      "grad_norm": 36112.42578125,
      "learning_rate": 0.0002469639468690702,
      "loss": 2.8775,
      "step": 560
    },
    {
      "epoch": 0.540796963946869,
      "grad_norm": 40120.73046875,
      "learning_rate": 0.00024601518026565465,
      "loss": 2.5516,
      "step": 570
    },
    {
      "epoch": 0.5502846299810247,
      "grad_norm": 53883.6640625,
      "learning_rate": 0.00024506641366223907,
      "loss": 2.6384,
      "step": 580
    },
    {
      "epoch": 0.5597722960151803,
      "grad_norm": 33984.17578125,
      "learning_rate": 0.0002441176470588235,
      "loss": 2.5499,
      "step": 590
    },
    {
      "epoch": 0.5692599620493358,
      "grad_norm": 43139.21484375,
      "learning_rate": 0.00024316888045540795,
      "loss": 2.8051,
      "step": 600
    },
    {
      "epoch": 0.5787476280834914,
      "grad_norm": 48365.90234375,
      "learning_rate": 0.0002422201138519924,
      "loss": 2.5045,
      "step": 610
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 53100.74609375,
      "learning_rate": 0.00024127134724857683,
      "loss": 2.5316,
      "step": 620
    },
    {
      "epoch": 0.5977229601518027,
      "grad_norm": 41934.68359375,
      "learning_rate": 0.00024032258064516125,
      "loss": 2.6017,
      "step": 630
    },
    {
      "epoch": 0.6072106261859582,
      "grad_norm": 32078.84765625,
      "learning_rate": 0.00023937381404174571,
      "loss": 2.62,
      "step": 640
    },
    {
      "epoch": 0.6166982922201139,
      "grad_norm": 52523.7578125,
      "learning_rate": 0.00023842504743833016,
      "loss": 2.5485,
      "step": 650
    },
    {
      "epoch": 0.6261859582542695,
      "grad_norm": 33042.6875,
      "learning_rate": 0.0002374762808349146,
      "loss": 2.6211,
      "step": 660
    },
    {
      "epoch": 0.635673624288425,
      "grad_norm": 36320.71484375,
      "learning_rate": 0.00023652751423149904,
      "loss": 2.6927,
      "step": 670
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 51923.28515625,
      "learning_rate": 0.00023557874762808348,
      "loss": 2.7255,
      "step": 680
    },
    {
      "epoch": 0.6546489563567363,
      "grad_norm": 58207.76171875,
      "learning_rate": 0.00023462998102466792,
      "loss": 2.8782,
      "step": 690
    },
    {
      "epoch": 0.6641366223908919,
      "grad_norm": 51462.17578125,
      "learning_rate": 0.00023368121442125236,
      "loss": 2.4841,
      "step": 700
    },
    {
      "epoch": 0.6736242884250474,
      "grad_norm": 49118.6484375,
      "learning_rate": 0.00023273244781783678,
      "loss": 2.797,
      "step": 710
    },
    {
      "epoch": 0.683111954459203,
      "grad_norm": 31429.275390625,
      "learning_rate": 0.00023178368121442122,
      "loss": 2.6122,
      "step": 720
    },
    {
      "epoch": 0.6925996204933587,
      "grad_norm": 46235.88671875,
      "learning_rate": 0.00023083491461100566,
      "loss": 2.5496,
      "step": 730
    },
    {
      "epoch": 0.7020872865275142,
      "grad_norm": 44399.54296875,
      "learning_rate": 0.0002298861480075901,
      "loss": 2.6035,
      "step": 740
    },
    {
      "epoch": 0.7115749525616698,
      "grad_norm": 54712.93359375,
      "learning_rate": 0.00022893738140417457,
      "loss": 2.5971,
      "step": 750
    },
    {
      "epoch": 0.7210626185958254,
      "grad_norm": 71849.578125,
      "learning_rate": 0.000227988614800759,
      "loss": 2.8146,
      "step": 760
    },
    {
      "epoch": 0.7305502846299811,
      "grad_norm": 28588.4453125,
      "learning_rate": 0.00022703984819734345,
      "loss": 2.4522,
      "step": 770
    },
    {
      "epoch": 0.7400379506641366,
      "grad_norm": 48430.8125,
      "learning_rate": 0.0002260910815939279,
      "loss": 2.5185,
      "step": 780
    },
    {
      "epoch": 0.7495256166982922,
      "grad_norm": 62622.03515625,
      "learning_rate": 0.0002251423149905123,
      "loss": 2.7323,
      "step": 790
    },
    {
      "epoch": 0.7590132827324478,
      "grad_norm": 29723.611328125,
      "learning_rate": 0.00022419354838709675,
      "loss": 2.6328,
      "step": 800
    },
    {
      "epoch": 0.7685009487666035,
      "grad_norm": 37353.69921875,
      "learning_rate": 0.0002232447817836812,
      "loss": 2.5116,
      "step": 810
    },
    {
      "epoch": 0.777988614800759,
      "grad_norm": 27261.109375,
      "learning_rate": 0.00022229601518026563,
      "loss": 2.3822,
      "step": 820
    },
    {
      "epoch": 0.7874762808349146,
      "grad_norm": 48481.91015625,
      "learning_rate": 0.00022134724857685007,
      "loss": 2.6031,
      "step": 830
    },
    {
      "epoch": 0.7969639468690702,
      "grad_norm": 67423.5234375,
      "learning_rate": 0.0002203984819734345,
      "loss": 2.5134,
      "step": 840
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 38647.5234375,
      "learning_rate": 0.00021944971537001895,
      "loss": 2.8003,
      "step": 850
    },
    {
      "epoch": 0.8159392789373814,
      "grad_norm": 44392.046875,
      "learning_rate": 0.00021850094876660342,
      "loss": 2.3719,
      "step": 860
    },
    {
      "epoch": 0.825426944971537,
      "grad_norm": 44738.7421875,
      "learning_rate": 0.00021755218216318786,
      "loss": 2.7654,
      "step": 870
    },
    {
      "epoch": 0.8349146110056926,
      "grad_norm": 50119.94140625,
      "learning_rate": 0.00021660341555977228,
      "loss": 2.8016,
      "step": 880
    },
    {
      "epoch": 0.8444022770398482,
      "grad_norm": 44784.31640625,
      "learning_rate": 0.00021565464895635672,
      "loss": 2.7616,
      "step": 890
    },
    {
      "epoch": 0.8538899430740038,
      "grad_norm": 38396.875,
      "learning_rate": 0.00021470588235294116,
      "loss": 2.3606,
      "step": 900
    },
    {
      "epoch": 0.8633776091081594,
      "grad_norm": 44417.20703125,
      "learning_rate": 0.0002137571157495256,
      "loss": 2.7227,
      "step": 910
    },
    {
      "epoch": 0.872865275142315,
      "grad_norm": 58098.26953125,
      "learning_rate": 0.00021280834914611004,
      "loss": 2.8431,
      "step": 920
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 39244.91796875,
      "learning_rate": 0.00021185958254269448,
      "loss": 2.6659,
      "step": 930
    },
    {
      "epoch": 0.8918406072106262,
      "grad_norm": 48443.859375,
      "learning_rate": 0.00021091081593927892,
      "loss": 2.6676,
      "step": 940
    },
    {
      "epoch": 0.9013282732447818,
      "grad_norm": 34485.37890625,
      "learning_rate": 0.00020996204933586334,
      "loss": 2.5023,
      "step": 950
    },
    {
      "epoch": 0.9108159392789373,
      "grad_norm": 51009.23046875,
      "learning_rate": 0.00020901328273244778,
      "loss": 2.4266,
      "step": 960
    },
    {
      "epoch": 0.920303605313093,
      "grad_norm": 52575.55859375,
      "learning_rate": 0.00020806451612903225,
      "loss": 2.6411,
      "step": 970
    },
    {
      "epoch": 0.9297912713472486,
      "grad_norm": 62793.13671875,
      "learning_rate": 0.0002071157495256167,
      "loss": 2.5471,
      "step": 980
    },
    {
      "epoch": 0.9392789373814042,
      "grad_norm": 36921.1171875,
      "learning_rate": 0.00020616698292220113,
      "loss": 2.3199,
      "step": 990
    },
    {
      "epoch": 0.9487666034155597,
      "grad_norm": 53267.47265625,
      "learning_rate": 0.00020521821631878557,
      "loss": 2.531,
      "step": 1000
    },
    {
      "epoch": 0.9582542694497154,
      "grad_norm": 48005.2734375,
      "learning_rate": 0.00020426944971537,
      "loss": 2.7004,
      "step": 1010
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 62444.7734375,
      "learning_rate": 0.00020332068311195445,
      "loss": 2.512,
      "step": 1020
    },
    {
      "epoch": 0.9772296015180265,
      "grad_norm": 59836.7109375,
      "learning_rate": 0.00020237191650853887,
      "loss": 2.4351,
      "step": 1030
    },
    {
      "epoch": 0.9867172675521821,
      "grad_norm": 40573.78125,
      "learning_rate": 0.0002014231499051233,
      "loss": 2.4179,
      "step": 1040
    },
    {
      "epoch": 0.9962049335863378,
      "grad_norm": 44080.7578125,
      "learning_rate": 0.00020047438330170775,
      "loss": 2.398,
      "step": 1050
    },
    {
      "epoch": 1.0056925996204933,
      "grad_norm": 33877.45703125,
      "learning_rate": 0.0001995256166982922,
      "loss": 2.3682,
      "step": 1060
    },
    {
      "epoch": 1.015180265654649,
      "grad_norm": 51870.8203125,
      "learning_rate": 0.00019857685009487663,
      "loss": 2.7123,
      "step": 1070
    },
    {
      "epoch": 1.0246679316888045,
      "grad_norm": 51773.44140625,
      "learning_rate": 0.0001976280834914611,
      "loss": 2.2736,
      "step": 1080
    },
    {
      "epoch": 1.0341555977229602,
      "grad_norm": 56711.31640625,
      "learning_rate": 0.00019667931688804554,
      "loss": 2.5197,
      "step": 1090
    },
    {
      "epoch": 1.0436432637571158,
      "grad_norm": 59257.33203125,
      "learning_rate": 0.00019573055028462998,
      "loss": 2.6219,
      "step": 1100
    },
    {
      "epoch": 1.0531309297912714,
      "grad_norm": 53846.19140625,
      "learning_rate": 0.00019478178368121442,
      "loss": 2.4326,
      "step": 1110
    },
    {
      "epoch": 1.0626185958254268,
      "grad_norm": 36457.6953125,
      "learning_rate": 0.00019383301707779884,
      "loss": 2.4285,
      "step": 1120
    },
    {
      "epoch": 1.0721062618595825,
      "grad_norm": 35792.48828125,
      "learning_rate": 0.00019288425047438328,
      "loss": 2.5142,
      "step": 1130
    },
    {
      "epoch": 1.081593927893738,
      "grad_norm": 48698.29296875,
      "learning_rate": 0.00019193548387096772,
      "loss": 2.5898,
      "step": 1140
    },
    {
      "epoch": 1.0910815939278937,
      "grad_norm": 49421.515625,
      "learning_rate": 0.00019098671726755216,
      "loss": 2.4373,
      "step": 1150
    },
    {
      "epoch": 1.1005692599620494,
      "grad_norm": 55340.50390625,
      "learning_rate": 0.0001900379506641366,
      "loss": 2.5662,
      "step": 1160
    },
    {
      "epoch": 1.110056925996205,
      "grad_norm": 48090.39453125,
      "learning_rate": 0.00018908918406072104,
      "loss": 2.5036,
      "step": 1170
    },
    {
      "epoch": 1.1195445920303606,
      "grad_norm": 44876.734375,
      "learning_rate": 0.00018814041745730546,
      "loss": 2.4786,
      "step": 1180
    },
    {
      "epoch": 1.129032258064516,
      "grad_norm": 32191.291015625,
      "learning_rate": 0.00018719165085388995,
      "loss": 2.3691,
      "step": 1190
    },
    {
      "epoch": 1.1385199240986716,
      "grad_norm": 45393.50390625,
      "learning_rate": 0.00018624288425047437,
      "loss": 2.3773,
      "step": 1200
    },
    {
      "epoch": 1.1480075901328273,
      "grad_norm": 41112.25,
      "learning_rate": 0.0001852941176470588,
      "loss": 2.3651,
      "step": 1210
    },
    {
      "epoch": 1.157495256166983,
      "grad_norm": 60757.26953125,
      "learning_rate": 0.00018434535104364325,
      "loss": 2.5137,
      "step": 1220
    },
    {
      "epoch": 1.1669829222011385,
      "grad_norm": 66891.90625,
      "learning_rate": 0.0001833965844402277,
      "loss": 2.509,
      "step": 1230
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 49964.94921875,
      "learning_rate": 0.00018244781783681213,
      "loss": 2.4245,
      "step": 1240
    },
    {
      "epoch": 1.1859582542694498,
      "grad_norm": 40843.953125,
      "learning_rate": 0.00018149905123339657,
      "loss": 2.5135,
      "step": 1250
    },
    {
      "epoch": 1.1954459203036052,
      "grad_norm": 41526.94921875,
      "learning_rate": 0.000180550284629981,
      "loss": 2.462,
      "step": 1260
    },
    {
      "epoch": 1.2049335863377608,
      "grad_norm": 61746.35546875,
      "learning_rate": 0.00017960151802656543,
      "loss": 2.5896,
      "step": 1270
    },
    {
      "epoch": 1.2144212523719164,
      "grad_norm": 70181.8359375,
      "learning_rate": 0.00017865275142314987,
      "loss": 2.5316,
      "step": 1280
    },
    {
      "epoch": 1.223908918406072,
      "grad_norm": 53246.8984375,
      "learning_rate": 0.00017770398481973434,
      "loss": 2.6413,
      "step": 1290
    },
    {
      "epoch": 1.2333965844402277,
      "grad_norm": 73111.1875,
      "learning_rate": 0.00017675521821631878,
      "loss": 2.8317,
      "step": 1300
    },
    {
      "epoch": 1.2428842504743833,
      "grad_norm": 66903.9765625,
      "learning_rate": 0.00017580645161290322,
      "loss": 2.4811,
      "step": 1310
    },
    {
      "epoch": 1.252371916508539,
      "grad_norm": 62090.640625,
      "learning_rate": 0.00017485768500948766,
      "loss": 2.2929,
      "step": 1320
    },
    {
      "epoch": 1.2618595825426944,
      "grad_norm": 67642.828125,
      "learning_rate": 0.0001739089184060721,
      "loss": 2.3664,
      "step": 1330
    },
    {
      "epoch": 1.2713472485768502,
      "grad_norm": 71350.453125,
      "learning_rate": 0.00017296015180265654,
      "loss": 2.6982,
      "step": 1340
    },
    {
      "epoch": 1.2808349146110056,
      "grad_norm": 68868.59375,
      "learning_rate": 0.00017201138519924096,
      "loss": 2.4653,
      "step": 1350
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 34398.10546875,
      "learning_rate": 0.0001710626185958254,
      "loss": 2.8469,
      "step": 1360
    },
    {
      "epoch": 1.2998102466793169,
      "grad_norm": 28785.814453125,
      "learning_rate": 0.00017011385199240984,
      "loss": 2.4446,
      "step": 1370
    },
    {
      "epoch": 1.3092979127134725,
      "grad_norm": 63315.2734375,
      "learning_rate": 0.00016916508538899428,
      "loss": 2.7553,
      "step": 1380
    },
    {
      "epoch": 1.3187855787476281,
      "grad_norm": 33551.16015625,
      "learning_rate": 0.00016821631878557872,
      "loss": 2.5087,
      "step": 1390
    },
    {
      "epoch": 1.3282732447817835,
      "grad_norm": 70521.0,
      "learning_rate": 0.0001672675521821632,
      "loss": 2.2943,
      "step": 1400
    },
    {
      "epoch": 1.3377609108159394,
      "grad_norm": 46276.01171875,
      "learning_rate": 0.00016631878557874763,
      "loss": 2.5293,
      "step": 1410
    },
    {
      "epoch": 1.3472485768500948,
      "grad_norm": 67903.2421875,
      "learning_rate": 0.00016537001897533207,
      "loss": 2.6694,
      "step": 1420
    },
    {
      "epoch": 1.3567362428842504,
      "grad_norm": 70050.4453125,
      "learning_rate": 0.0001644212523719165,
      "loss": 2.5332,
      "step": 1430
    },
    {
      "epoch": 1.366223908918406,
      "grad_norm": 72887.8515625,
      "learning_rate": 0.00016347248576850093,
      "loss": 2.515,
      "step": 1440
    },
    {
      "epoch": 1.3757115749525617,
      "grad_norm": 50361.84375,
      "learning_rate": 0.00016252371916508537,
      "loss": 2.3686,
      "step": 1450
    },
    {
      "epoch": 1.3851992409867173,
      "grad_norm": 49213.8671875,
      "learning_rate": 0.0001615749525616698,
      "loss": 2.1344,
      "step": 1460
    },
    {
      "epoch": 1.394686907020873,
      "grad_norm": 45530.61328125,
      "learning_rate": 0.00016062618595825425,
      "loss": 2.4642,
      "step": 1470
    },
    {
      "epoch": 1.4041745730550286,
      "grad_norm": 51126.44140625,
      "learning_rate": 0.0001596774193548387,
      "loss": 2.3413,
      "step": 1480
    },
    {
      "epoch": 1.413662239089184,
      "grad_norm": 65844.3515625,
      "learning_rate": 0.00015872865275142313,
      "loss": 2.6303,
      "step": 1490
    },
    {
      "epoch": 1.4231499051233396,
      "grad_norm": 55615.828125,
      "learning_rate": 0.00015777988614800757,
      "loss": 2.6791,
      "step": 1500
    },
    {
      "epoch": 1.4326375711574952,
      "grad_norm": 47457.8515625,
      "learning_rate": 0.00015683111954459204,
      "loss": 2.2863,
      "step": 1510
    },
    {
      "epoch": 1.4421252371916509,
      "grad_norm": 43538.76171875,
      "learning_rate": 0.00015588235294117646,
      "loss": 2.3739,
      "step": 1520
    },
    {
      "epoch": 1.4516129032258065,
      "grad_norm": 55471.94140625,
      "learning_rate": 0.0001549335863377609,
      "loss": 2.614,
      "step": 1530
    },
    {
      "epoch": 1.4611005692599621,
      "grad_norm": 50058.9453125,
      "learning_rate": 0.00015398481973434534,
      "loss": 2.7232,
      "step": 1540
    },
    {
      "epoch": 1.4705882352941178,
      "grad_norm": 57540.48828125,
      "learning_rate": 0.00015303605313092978,
      "loss": 2.691,
      "step": 1550
    },
    {
      "epoch": 1.4800759013282732,
      "grad_norm": 74039.6875,
      "learning_rate": 0.00015208728652751422,
      "loss": 2.5154,
      "step": 1560
    },
    {
      "epoch": 1.4895635673624288,
      "grad_norm": 32503.421875,
      "learning_rate": 0.00015113851992409866,
      "loss": 2.4158,
      "step": 1570
    },
    {
      "epoch": 1.4990512333965844,
      "grad_norm": 64400.4453125,
      "learning_rate": 0.0001501897533206831,
      "loss": 2.4556,
      "step": 1580
    },
    {
      "epoch": 1.50853889943074,
      "grad_norm": 47418.6875,
      "learning_rate": 0.00014924098671726755,
      "loss": 2.3813,
      "step": 1590
    },
    {
      "epoch": 1.5180265654648957,
      "grad_norm": 47791.46875,
      "learning_rate": 0.00014829222011385199,
      "loss": 2.3875,
      "step": 1600
    },
    {
      "epoch": 1.527514231499051,
      "grad_norm": 74759.0234375,
      "learning_rate": 0.00014734345351043643,
      "loss": 2.3982,
      "step": 1610
    },
    {
      "epoch": 1.537001897533207,
      "grad_norm": 62083.11328125,
      "learning_rate": 0.00014639468690702087,
      "loss": 2.5973,
      "step": 1620
    },
    {
      "epoch": 1.5464895635673623,
      "grad_norm": 49129.4375,
      "learning_rate": 0.00014544592030360528,
      "loss": 2.5247,
      "step": 1630
    },
    {
      "epoch": 1.5559772296015182,
      "grad_norm": 46201.140625,
      "learning_rate": 0.00014449715370018972,
      "loss": 2.3515,
      "step": 1640
    },
    {
      "epoch": 1.5654648956356736,
      "grad_norm": 37917.08203125,
      "learning_rate": 0.0001435483870967742,
      "loss": 2.5954,
      "step": 1650
    },
    {
      "epoch": 1.5749525616698292,
      "grad_norm": 30253.0078125,
      "learning_rate": 0.00014259962049335863,
      "loss": 2.3636,
      "step": 1660
    },
    {
      "epoch": 1.5844402277039848,
      "grad_norm": 41582.8515625,
      "learning_rate": 0.00014165085388994307,
      "loss": 2.5871,
      "step": 1670
    },
    {
      "epoch": 1.5939278937381403,
      "grad_norm": 46132.125,
      "learning_rate": 0.0001407020872865275,
      "loss": 2.4314,
      "step": 1680
    },
    {
      "epoch": 1.603415559772296,
      "grad_norm": 44335.89453125,
      "learning_rate": 0.00013975332068311193,
      "loss": 2.5476,
      "step": 1690
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 51315.51171875,
      "learning_rate": 0.0001388045540796964,
      "loss": 2.5659,
      "step": 1700
    },
    {
      "epoch": 1.6223908918406074,
      "grad_norm": 45760.8671875,
      "learning_rate": 0.00013785578747628084,
      "loss": 2.3172,
      "step": 1710
    },
    {
      "epoch": 1.6318785578747628,
      "grad_norm": 35739.67578125,
      "learning_rate": 0.00013690702087286525,
      "loss": 2.1198,
      "step": 1720
    },
    {
      "epoch": 1.6413662239089184,
      "grad_norm": 37215.8203125,
      "learning_rate": 0.0001359582542694497,
      "loss": 2.452,
      "step": 1730
    },
    {
      "epoch": 1.650853889943074,
      "grad_norm": 64394.03125,
      "learning_rate": 0.00013500948766603414,
      "loss": 2.3707,
      "step": 1740
    },
    {
      "epoch": 1.6603415559772297,
      "grad_norm": 43542.015625,
      "learning_rate": 0.0001340607210626186,
      "loss": 2.5398,
      "step": 1750
    },
    {
      "epoch": 1.6698292220113853,
      "grad_norm": 50323.6796875,
      "learning_rate": 0.00013311195445920302,
      "loss": 2.4157,
      "step": 1760
    },
    {
      "epoch": 1.6793168880455407,
      "grad_norm": 50628.125,
      "learning_rate": 0.00013216318785578746,
      "loss": 2.6317,
      "step": 1770
    },
    {
      "epoch": 1.6888045540796965,
      "grad_norm": 57771.44140625,
      "learning_rate": 0.0001312144212523719,
      "loss": 2.8734,
      "step": 1780
    },
    {
      "epoch": 1.698292220113852,
      "grad_norm": 60223.15234375,
      "learning_rate": 0.00013026565464895634,
      "loss": 2.3269,
      "step": 1790
    },
    {
      "epoch": 1.7077798861480076,
      "grad_norm": 47362.53125,
      "learning_rate": 0.00012931688804554078,
      "loss": 2.2605,
      "step": 1800
    },
    {
      "epoch": 1.7172675521821632,
      "grad_norm": 37954.03515625,
      "learning_rate": 0.00012836812144212522,
      "loss": 2.3191,
      "step": 1810
    },
    {
      "epoch": 1.7267552182163188,
      "grad_norm": 43025.02734375,
      "learning_rate": 0.00012741935483870967,
      "loss": 2.3735,
      "step": 1820
    },
    {
      "epoch": 1.7362428842504745,
      "grad_norm": 41558.46875,
      "learning_rate": 0.0001264705882352941,
      "loss": 2.293,
      "step": 1830
    },
    {
      "epoch": 1.7457305502846299,
      "grad_norm": 49129.84765625,
      "learning_rate": 0.00012552182163187855,
      "loss": 2.1937,
      "step": 1840
    },
    {
      "epoch": 1.7552182163187857,
      "grad_norm": 70408.78125,
      "learning_rate": 0.000124573055028463,
      "loss": 2.2823,
      "step": 1850
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 62529.5546875,
      "learning_rate": 0.00012362428842504743,
      "loss": 2.5064,
      "step": 1860
    },
    {
      "epoch": 1.7741935483870968,
      "grad_norm": 49857.75,
      "learning_rate": 0.00012267552182163187,
      "loss": 2.4747,
      "step": 1870
    },
    {
      "epoch": 1.7836812144212524,
      "grad_norm": 51181.6640625,
      "learning_rate": 0.00012172675521821631,
      "loss": 2.0885,
      "step": 1880
    },
    {
      "epoch": 1.793168880455408,
      "grad_norm": 46104.77734375,
      "learning_rate": 0.00012077798861480075,
      "loss": 2.2108,
      "step": 1890
    },
    {
      "epoch": 1.8026565464895636,
      "grad_norm": 68851.359375,
      "learning_rate": 0.00011982922201138518,
      "loss": 2.2927,
      "step": 1900
    },
    {
      "epoch": 1.812144212523719,
      "grad_norm": 64206.4453125,
      "learning_rate": 0.00011888045540796962,
      "loss": 2.5849,
      "step": 1910
    },
    {
      "epoch": 1.821631878557875,
      "grad_norm": 64356.98046875,
      "learning_rate": 0.00011793168880455408,
      "loss": 2.1795,
      "step": 1920
    },
    {
      "epoch": 1.8311195445920303,
      "grad_norm": 48031.40234375,
      "learning_rate": 0.00011698292220113852,
      "loss": 2.452,
      "step": 1930
    },
    {
      "epoch": 1.840607210626186,
      "grad_norm": 53027.03125,
      "learning_rate": 0.00011603415559772295,
      "loss": 2.2552,
      "step": 1940
    },
    {
      "epoch": 1.8500948766603416,
      "grad_norm": 49272.66015625,
      "learning_rate": 0.00011508538899430739,
      "loss": 2.4025,
      "step": 1950
    },
    {
      "epoch": 1.8595825426944972,
      "grad_norm": 49336.984375,
      "learning_rate": 0.00011413662239089183,
      "loss": 2.4446,
      "step": 1960
    },
    {
      "epoch": 1.8690702087286528,
      "grad_norm": 53901.43359375,
      "learning_rate": 0.00011318785578747628,
      "loss": 2.6126,
      "step": 1970
    },
    {
      "epoch": 1.8785578747628082,
      "grad_norm": 46691.5,
      "learning_rate": 0.00011223908918406071,
      "loss": 2.4412,
      "step": 1980
    },
    {
      "epoch": 1.888045540796964,
      "grad_norm": 52092.82421875,
      "learning_rate": 0.00011129032258064515,
      "loss": 2.1675,
      "step": 1990
    },
    {
      "epoch": 1.8975332068311195,
      "grad_norm": 62626.73046875,
      "learning_rate": 0.00011034155597722959,
      "loss": 2.4588,
      "step": 2000
    },
    {
      "epoch": 1.907020872865275,
      "grad_norm": 64175.49609375,
      "learning_rate": 0.00010939278937381402,
      "loss": 2.4859,
      "step": 2010
    },
    {
      "epoch": 1.9165085388994307,
      "grad_norm": 40805.48828125,
      "learning_rate": 0.00010844402277039846,
      "loss": 2.5355,
      "step": 2020
    },
    {
      "epoch": 1.9259962049335864,
      "grad_norm": 75236.2734375,
      "learning_rate": 0.00010749525616698292,
      "loss": 2.6079,
      "step": 2030
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 50977.046875,
      "learning_rate": 0.00010654648956356736,
      "loss": 2.8004,
      "step": 2040
    },
    {
      "epoch": 1.9449715370018974,
      "grad_norm": 68421.0078125,
      "learning_rate": 0.0001055977229601518,
      "loss": 2.4265,
      "step": 2050
    },
    {
      "epoch": 1.9544592030360532,
      "grad_norm": 49354.4453125,
      "learning_rate": 0.00010464895635673623,
      "loss": 2.288,
      "step": 2060
    },
    {
      "epoch": 1.9639468690702087,
      "grad_norm": 51897.671875,
      "learning_rate": 0.00010370018975332067,
      "loss": 2.3396,
      "step": 2070
    },
    {
      "epoch": 1.9734345351043643,
      "grad_norm": 61900.73046875,
      "learning_rate": 0.00010275142314990512,
      "loss": 2.355,
      "step": 2080
    },
    {
      "epoch": 1.98292220113852,
      "grad_norm": 52437.09765625,
      "learning_rate": 0.00010180265654648956,
      "loss": 2.5093,
      "step": 2090
    },
    {
      "epoch": 1.9924098671726755,
      "grad_norm": 46924.77734375,
      "learning_rate": 0.00010085388994307399,
      "loss": 2.3031,
      "step": 2100
    },
    {
      "epoch": 2.001897533206831,
      "grad_norm": 68028.7109375,
      "learning_rate": 9.990512333965843e-05,
      "loss": 2.1892,
      "step": 2110
    },
    {
      "epoch": 2.0113851992409866,
      "grad_norm": 61523.08984375,
      "learning_rate": 9.895635673624287e-05,
      "loss": 2.3585,
      "step": 2120
    },
    {
      "epoch": 2.0208728652751424,
      "grad_norm": 34276.39453125,
      "learning_rate": 9.800759013282733e-05,
      "loss": 2.3185,
      "step": 2130
    },
    {
      "epoch": 2.030360531309298,
      "grad_norm": 59039.2578125,
      "learning_rate": 9.705882352941176e-05,
      "loss": 2.3707,
      "step": 2140
    },
    {
      "epoch": 2.0398481973434537,
      "grad_norm": 54099.70703125,
      "learning_rate": 9.61100569259962e-05,
      "loss": 2.4932,
      "step": 2150
    },
    {
      "epoch": 2.049335863377609,
      "grad_norm": 72479.53125,
      "learning_rate": 9.516129032258064e-05,
      "loss": 2.4833,
      "step": 2160
    },
    {
      "epoch": 2.0588235294117645,
      "grad_norm": 64506.16796875,
      "learning_rate": 9.421252371916507e-05,
      "loss": 2.4278,
      "step": 2170
    },
    {
      "epoch": 2.0683111954459203,
      "grad_norm": 59295.41015625,
      "learning_rate": 9.326375711574951e-05,
      "loss": 2.708,
      "step": 2180
    },
    {
      "epoch": 2.0777988614800758,
      "grad_norm": 77956.875,
      "learning_rate": 9.231499051233396e-05,
      "loss": 2.3826,
      "step": 2190
    },
    {
      "epoch": 2.0872865275142316,
      "grad_norm": 73827.3984375,
      "learning_rate": 9.13662239089184e-05,
      "loss": 2.5028,
      "step": 2200
    },
    {
      "epoch": 2.096774193548387,
      "grad_norm": 54907.3515625,
      "learning_rate": 9.041745730550284e-05,
      "loss": 2.286,
      "step": 2210
    },
    {
      "epoch": 2.106261859582543,
      "grad_norm": 50990.41015625,
      "learning_rate": 8.946869070208727e-05,
      "loss": 2.164,
      "step": 2220
    },
    {
      "epoch": 2.1157495256166983,
      "grad_norm": 56339.9609375,
      "learning_rate": 8.851992409867171e-05,
      "loss": 2.2828,
      "step": 2230
    },
    {
      "epoch": 2.1252371916508537,
      "grad_norm": 51530.21875,
      "learning_rate": 8.757115749525617e-05,
      "loss": 2.2784,
      "step": 2240
    },
    {
      "epoch": 2.1347248576850095,
      "grad_norm": 66486.9609375,
      "learning_rate": 8.662239089184061e-05,
      "loss": 2.5066,
      "step": 2250
    },
    {
      "epoch": 2.144212523719165,
      "grad_norm": 70629.453125,
      "learning_rate": 8.567362428842504e-05,
      "loss": 2.528,
      "step": 2260
    },
    {
      "epoch": 2.153700189753321,
      "grad_norm": 48669.984375,
      "learning_rate": 8.472485768500948e-05,
      "loss": 2.5547,
      "step": 2270
    },
    {
      "epoch": 2.163187855787476,
      "grad_norm": 60575.18359375,
      "learning_rate": 8.377609108159392e-05,
      "loss": 2.4749,
      "step": 2280
    },
    {
      "epoch": 2.172675521821632,
      "grad_norm": 59207.5078125,
      "learning_rate": 8.282732447817835e-05,
      "loss": 2.4715,
      "step": 2290
    },
    {
      "epoch": 2.1821631878557874,
      "grad_norm": 63337.92578125,
      "learning_rate": 8.18785578747628e-05,
      "loss": 2.3694,
      "step": 2300
    },
    {
      "epoch": 2.191650853889943,
      "grad_norm": 72306.8828125,
      "learning_rate": 8.092979127134724e-05,
      "loss": 2.3877,
      "step": 2310
    },
    {
      "epoch": 2.2011385199240987,
      "grad_norm": 62194.671875,
      "learning_rate": 7.998102466793168e-05,
      "loss": 2.5005,
      "step": 2320
    },
    {
      "epoch": 2.210626185958254,
      "grad_norm": 60617.62109375,
      "learning_rate": 7.903225806451613e-05,
      "loss": 2.3723,
      "step": 2330
    },
    {
      "epoch": 2.22011385199241,
      "grad_norm": 76398.9296875,
      "learning_rate": 7.808349146110055e-05,
      "loss": 2.3277,
      "step": 2340
    },
    {
      "epoch": 2.2296015180265654,
      "grad_norm": 60773.3046875,
      "learning_rate": 7.713472485768501e-05,
      "loss": 2.1855,
      "step": 2350
    },
    {
      "epoch": 2.239089184060721,
      "grad_norm": 69989.3203125,
      "learning_rate": 7.618595825426945e-05,
      "loss": 2.4943,
      "step": 2360
    },
    {
      "epoch": 2.2485768500948766,
      "grad_norm": 54384.9375,
      "learning_rate": 7.523719165085389e-05,
      "loss": 2.4533,
      "step": 2370
    },
    {
      "epoch": 2.258064516129032,
      "grad_norm": 40242.40625,
      "learning_rate": 7.428842504743832e-05,
      "loss": 2.1624,
      "step": 2380
    },
    {
      "epoch": 2.267552182163188,
      "grad_norm": 66937.71875,
      "learning_rate": 7.333965844402277e-05,
      "loss": 2.46,
      "step": 2390
    },
    {
      "epoch": 2.2770398481973433,
      "grad_norm": 52563.6953125,
      "learning_rate": 7.23908918406072e-05,
      "loss": 2.6633,
      "step": 2400
    },
    {
      "epoch": 2.286527514231499,
      "grad_norm": 71685.1953125,
      "learning_rate": 7.144212523719164e-05,
      "loss": 2.2915,
      "step": 2410
    },
    {
      "epoch": 2.2960151802656545,
      "grad_norm": 43494.375,
      "learning_rate": 7.049335863377608e-05,
      "loss": 2.3352,
      "step": 2420
    },
    {
      "epoch": 2.3055028462998104,
      "grad_norm": 67135.2890625,
      "learning_rate": 6.954459203036052e-05,
      "loss": 2.7505,
      "step": 2430
    },
    {
      "epoch": 2.314990512333966,
      "grad_norm": 54290.98828125,
      "learning_rate": 6.859582542694496e-05,
      "loss": 2.5974,
      "step": 2440
    },
    {
      "epoch": 2.324478178368121,
      "grad_norm": 56419.82421875,
      "learning_rate": 6.76470588235294e-05,
      "loss": 2.2805,
      "step": 2450
    },
    {
      "epoch": 2.333965844402277,
      "grad_norm": 35730.01171875,
      "learning_rate": 6.669829222011385e-05,
      "loss": 2.3744,
      "step": 2460
    },
    {
      "epoch": 2.3434535104364325,
      "grad_norm": 75778.3515625,
      "learning_rate": 6.574952561669829e-05,
      "loss": 2.5537,
      "step": 2470
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 50329.4296875,
      "learning_rate": 6.480075901328273e-05,
      "loss": 2.1952,
      "step": 2480
    },
    {
      "epoch": 2.3624288425047437,
      "grad_norm": 53089.6953125,
      "learning_rate": 6.385199240986717e-05,
      "loss": 2.0763,
      "step": 2490
    },
    {
      "epoch": 2.3719165085388996,
      "grad_norm": 79240.890625,
      "learning_rate": 6.290322580645161e-05,
      "loss": 2.484,
      "step": 2500
    },
    {
      "epoch": 2.381404174573055,
      "grad_norm": 43913.6875,
      "learning_rate": 6.195445920303605e-05,
      "loss": 2.3439,
      "step": 2510
    },
    {
      "epoch": 2.3908918406072104,
      "grad_norm": 57772.84765625,
      "learning_rate": 6.100569259962049e-05,
      "loss": 2.3812,
      "step": 2520
    },
    {
      "epoch": 2.4003795066413662,
      "grad_norm": 62002.19921875,
      "learning_rate": 6.005692599620493e-05,
      "loss": 2.4135,
      "step": 2530
    },
    {
      "epoch": 2.4098671726755216,
      "grad_norm": 39946.08984375,
      "learning_rate": 5.910815939278937e-05,
      "loss": 2.5643,
      "step": 2540
    },
    {
      "epoch": 2.4193548387096775,
      "grad_norm": 66098.4921875,
      "learning_rate": 5.815939278937381e-05,
      "loss": 2.4055,
      "step": 2550
    },
    {
      "epoch": 2.428842504743833,
      "grad_norm": 50140.83984375,
      "learning_rate": 5.721062618595825e-05,
      "loss": 2.5077,
      "step": 2560
    },
    {
      "epoch": 2.4383301707779887,
      "grad_norm": 49406.296875,
      "learning_rate": 5.6261859582542687e-05,
      "loss": 2.3301,
      "step": 2570
    },
    {
      "epoch": 2.447817836812144,
      "grad_norm": 60008.83984375,
      "learning_rate": 5.5313092979127134e-05,
      "loss": 2.5025,
      "step": 2580
    },
    {
      "epoch": 2.4573055028462996,
      "grad_norm": 58100.9609375,
      "learning_rate": 5.436432637571157e-05,
      "loss": 2.2292,
      "step": 2590
    },
    {
      "epoch": 2.4667931688804554,
      "grad_norm": 49435.859375,
      "learning_rate": 5.341555977229601e-05,
      "loss": 2.1378,
      "step": 2600
    },
    {
      "epoch": 2.476280834914611,
      "grad_norm": 45516.86328125,
      "learning_rate": 5.246679316888045e-05,
      "loss": 2.2158,
      "step": 2610
    },
    {
      "epoch": 2.4857685009487667,
      "grad_norm": 76934.640625,
      "learning_rate": 5.151802656546489e-05,
      "loss": 2.3604,
      "step": 2620
    },
    {
      "epoch": 2.495256166982922,
      "grad_norm": 64938.0625,
      "learning_rate": 5.0569259962049334e-05,
      "loss": 2.12,
      "step": 2630
    },
    {
      "epoch": 2.504743833017078,
      "grad_norm": 47184.97265625,
      "learning_rate": 4.9620493358633775e-05,
      "loss": 2.2371,
      "step": 2640
    },
    {
      "epoch": 2.5142314990512333,
      "grad_norm": 48850.3359375,
      "learning_rate": 4.867172675521821e-05,
      "loss": 2.3147,
      "step": 2650
    },
    {
      "epoch": 2.5237191650853887,
      "grad_norm": 57147.171875,
      "learning_rate": 4.772296015180266e-05,
      "loss": 2.6587,
      "step": 2660
    },
    {
      "epoch": 2.5332068311195446,
      "grad_norm": 44859.42578125,
      "learning_rate": 4.677419354838709e-05,
      "loss": 2.2769,
      "step": 2670
    },
    {
      "epoch": 2.5426944971537004,
      "grad_norm": 42058.71484375,
      "learning_rate": 4.582542694497153e-05,
      "loss": 2.199,
      "step": 2680
    },
    {
      "epoch": 2.552182163187856,
      "grad_norm": 47817.421875,
      "learning_rate": 4.4876660341555974e-05,
      "loss": 2.3084,
      "step": 2690
    },
    {
      "epoch": 2.5616698292220113,
      "grad_norm": 57066.29296875,
      "learning_rate": 4.3927893738140415e-05,
      "loss": 2.2844,
      "step": 2700
    },
    {
      "epoch": 2.571157495256167,
      "grad_norm": 61636.140625,
      "learning_rate": 4.297912713472485e-05,
      "loss": 2.7867,
      "step": 2710
    },
    {
      "epoch": 2.5806451612903225,
      "grad_norm": 56591.83203125,
      "learning_rate": 4.20303605313093e-05,
      "loss": 2.5439,
      "step": 2720
    },
    {
      "epoch": 2.590132827324478,
      "grad_norm": 59680.15625,
      "learning_rate": 4.108159392789373e-05,
      "loss": 2.553,
      "step": 2730
    },
    {
      "epoch": 2.5996204933586338,
      "grad_norm": 83471.6171875,
      "learning_rate": 4.013282732447818e-05,
      "loss": 2.3089,
      "step": 2740
    },
    {
      "epoch": 2.6091081593927896,
      "grad_norm": 45257.11328125,
      "learning_rate": 3.9184060721062614e-05,
      "loss": 2.4856,
      "step": 2750
    },
    {
      "epoch": 2.618595825426945,
      "grad_norm": 50609.31640625,
      "learning_rate": 3.8235294117647055e-05,
      "loss": 2.2182,
      "step": 2760
    },
    {
      "epoch": 2.6280834914611004,
      "grad_norm": 51633.08203125,
      "learning_rate": 3.72865275142315e-05,
      "loss": 2.3591,
      "step": 2770
    },
    {
      "epoch": 2.6375711574952563,
      "grad_norm": 60178.5625,
      "learning_rate": 3.633776091081594e-05,
      "loss": 2.3472,
      "step": 2780
    },
    {
      "epoch": 2.6470588235294117,
      "grad_norm": 66295.890625,
      "learning_rate": 3.538899430740038e-05,
      "loss": 2.2221,
      "step": 2790
    },
    {
      "epoch": 2.656546489563567,
      "grad_norm": 46986.85546875,
      "learning_rate": 3.444022770398482e-05,
      "loss": 2.419,
      "step": 2800
    },
    {
      "epoch": 2.666034155597723,
      "grad_norm": 73273.2109375,
      "learning_rate": 3.3491461100569255e-05,
      "loss": 2.173,
      "step": 2810
    },
    {
      "epoch": 2.675521821631879,
      "grad_norm": 41186.96875,
      "learning_rate": 3.2542694497153696e-05,
      "loss": 2.2208,
      "step": 2820
    },
    {
      "epoch": 2.685009487666034,
      "grad_norm": 59700.21484375,
      "learning_rate": 3.159392789373814e-05,
      "loss": 2.5235,
      "step": 2830
    },
    {
      "epoch": 2.6944971537001896,
      "grad_norm": 61235.7265625,
      "learning_rate": 3.064516129032258e-05,
      "loss": 2.5583,
      "step": 2840
    },
    {
      "epoch": 2.7039848197343455,
      "grad_norm": 50577.8125,
      "learning_rate": 2.9696394686907016e-05,
      "loss": 2.3678,
      "step": 2850
    },
    {
      "epoch": 2.713472485768501,
      "grad_norm": 44482.1953125,
      "learning_rate": 2.8747628083491457e-05,
      "loss": 2.5621,
      "step": 2860
    },
    {
      "epoch": 2.7229601518026563,
      "grad_norm": 50254.0859375,
      "learning_rate": 2.77988614800759e-05,
      "loss": 2.2121,
      "step": 2870
    },
    {
      "epoch": 2.732447817836812,
      "grad_norm": 70511.6796875,
      "learning_rate": 2.6850094876660336e-05,
      "loss": 2.6848,
      "step": 2880
    },
    {
      "epoch": 2.741935483870968,
      "grad_norm": 58027.02734375,
      "learning_rate": 2.5901328273244777e-05,
      "loss": 2.5703,
      "step": 2890
    },
    {
      "epoch": 2.7514231499051234,
      "grad_norm": 44127.66015625,
      "learning_rate": 2.495256166982922e-05,
      "loss": 2.2678,
      "step": 2900
    },
    {
      "epoch": 2.760910815939279,
      "grad_norm": 64335.5546875,
      "learning_rate": 2.400379506641366e-05,
      "loss": 2.5511,
      "step": 2910
    },
    {
      "epoch": 2.7703984819734346,
      "grad_norm": 47779.03125,
      "learning_rate": 2.3055028462998097e-05,
      "loss": 2.2809,
      "step": 2920
    },
    {
      "epoch": 2.77988614800759,
      "grad_norm": 34427.80078125,
      "learning_rate": 2.210626185958254e-05,
      "loss": 2.2378,
      "step": 2930
    },
    {
      "epoch": 2.789373814041746,
      "grad_norm": 72031.765625,
      "learning_rate": 2.115749525616698e-05,
      "loss": 2.4056,
      "step": 2940
    },
    {
      "epoch": 2.7988614800759013,
      "grad_norm": 59049.859375,
      "learning_rate": 2.0208728652751424e-05,
      "loss": 2.4454,
      "step": 2950
    },
    {
      "epoch": 2.808349146110057,
      "grad_norm": 45918.90625,
      "learning_rate": 1.925996204933586e-05,
      "loss": 2.3305,
      "step": 2960
    },
    {
      "epoch": 2.8178368121442126,
      "grad_norm": 34349.4140625,
      "learning_rate": 1.83111954459203e-05,
      "loss": 2.4499,
      "step": 2970
    },
    {
      "epoch": 2.827324478178368,
      "grad_norm": 64738.36328125,
      "learning_rate": 1.736242884250474e-05,
      "loss": 2.5572,
      "step": 2980
    },
    {
      "epoch": 2.836812144212524,
      "grad_norm": 49798.4375,
      "learning_rate": 1.6413662239089182e-05,
      "loss": 2.4135,
      "step": 2990
    },
    {
      "epoch": 2.846299810246679,
      "grad_norm": 37151.46484375,
      "learning_rate": 1.5464895635673624e-05,
      "loss": 2.3791,
      "step": 3000
    },
    {
      "epoch": 2.855787476280835,
      "grad_norm": 66939.6015625,
      "learning_rate": 1.4516129032258063e-05,
      "loss": 2.427,
      "step": 3010
    },
    {
      "epoch": 2.8652751423149905,
      "grad_norm": 73412.0234375,
      "learning_rate": 1.3567362428842504e-05,
      "loss": 2.2852,
      "step": 3020
    },
    {
      "epoch": 2.8747628083491463,
      "grad_norm": 66597.625,
      "learning_rate": 1.2618595825426944e-05,
      "loss": 2.2649,
      "step": 3030
    },
    {
      "epoch": 2.8842504743833017,
      "grad_norm": 41420.515625,
      "learning_rate": 1.1669829222011385e-05,
      "loss": 2.4156,
      "step": 3040
    },
    {
      "epoch": 2.893738140417457,
      "grad_norm": 52702.265625,
      "learning_rate": 1.0721062618595824e-05,
      "loss": 2.7563,
      "step": 3050
    },
    {
      "epoch": 2.903225806451613,
      "grad_norm": 71378.125,
      "learning_rate": 9.772296015180266e-06,
      "loss": 2.5447,
      "step": 3060
    },
    {
      "epoch": 2.9127134724857684,
      "grad_norm": 45930.90625,
      "learning_rate": 8.823529411764705e-06,
      "loss": 2.5711,
      "step": 3070
    },
    {
      "epoch": 2.9222011385199242,
      "grad_norm": 53727.60546875,
      "learning_rate": 7.874762808349145e-06,
      "loss": 2.4456,
      "step": 3080
    },
    {
      "epoch": 2.9316888045540797,
      "grad_norm": 56523.80859375,
      "learning_rate": 6.925996204933586e-06,
      "loss": 2.4842,
      "step": 3090
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 57487.86328125,
      "learning_rate": 5.977229601518026e-06,
      "loss": 2.3486,
      "step": 3100
    },
    {
      "epoch": 2.950664136622391,
      "grad_norm": 43354.6484375,
      "learning_rate": 5.0284629981024665e-06,
      "loss": 2.2732,
      "step": 3110
    },
    {
      "epoch": 2.9601518026565463,
      "grad_norm": 42105.3046875,
      "learning_rate": 4.079696394686907e-06,
      "loss": 2.3445,
      "step": 3120
    },
    {
      "epoch": 2.969639468690702,
      "grad_norm": 62622.2734375,
      "learning_rate": 3.130929791271347e-06,
      "loss": 2.3291,
      "step": 3130
    },
    {
      "epoch": 2.9791271347248576,
      "grad_norm": 35525.8515625,
      "learning_rate": 2.182163187855787e-06,
      "loss": 2.0454,
      "step": 3140
    },
    {
      "epoch": 2.9886148007590134,
      "grad_norm": 67550.875,
      "learning_rate": 1.2333965844402276e-06,
      "loss": 2.3254,
      "step": 3150
    },
    {
      "epoch": 2.998102466793169,
      "grad_norm": 48876.59375,
      "learning_rate": 2.846299810246679e-07,
      "loss": 2.2959,
      "step": 3160
    }
  ],
  "logging_steps": 10,
  "max_steps": 3162,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.738418693603328e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
